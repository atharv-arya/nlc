{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRKHDi/Ni+rrveET9xt/Hg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Basic imports and initializations\n"
      ],
      "metadata": {
        "id": "yjtamXDn0CiZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMMcBX1BFWhO",
        "outputId": "346fdc8b-cd3e-4dc7-90e5-fd94d3349095"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scraping all pages: 100%|██████████| 1020/1020 [56:31<00:00,  3.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved to nairaland_bbnaija_2024.csv\n"
          ]
        }
      ],
      "source": [
        "# Importing all basic libs\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import time\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BBNaija 2024 thread base url\n",
        "BASE_URL = \"https://www.nairaland.com/8156758/bbnaija-2024-live-updates-thread\"\n",
        "\n",
        "# Custom headers to mimic a browser to prevent bot detection\n",
        "USER_AGENTS = [\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 ' +\n",
        "    '(KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 ' +\n",
        "    '(KHTML, like Gecko) Version/16.2 Safari/605.1.15',\n",
        "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 ' +\n",
        "    '(KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36'\n",
        "]\n"
      ],
      "metadata": {
        "id": "TQqD-O_CFjsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper function that will help us scrap data of one page"
      ],
      "metadata": {
        "id": "GbPAwt6O0Jwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_page(page_num):\n",
        "    url = BASE_URL if page_num == 1 else f\"{BASE_URL}/{page_num}\"\n",
        "    headers = {'User-Agent': random.choice(USER_AGENTS)}\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=15)\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Connection error on page {page_num}: {e}\")\n",
        "        return []\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch page {page_num}, Status Code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    posts = []\n",
        "\n",
        "    rows = soup.select('table[summary=\"posts\"] tr')\n",
        "    for i in range(0, len(rows), 2):\n",
        "        try:\n",
        "            meta_td = rows[i].find('td', class_='bold l pu')\n",
        "            if not meta_td:\n",
        "                continue\n",
        "\n",
        "            username_tag = meta_td.find('a', class_='user')\n",
        "            username = username_tag.text.strip() if username_tag else \"\"\n",
        "\n",
        "            timestamp_parts = meta_td.find_all('b')\n",
        "            timestamp = ' '.join([b.text.strip() for b in timestamp_parts]) if timestamp_parts else \"\"\n",
        "\n",
        "            gender = \"\"\n",
        "\n",
        "            content_td = rows[i+1].find('td', class_='l w pd')\n",
        "            content_div = content_td.find('div', class_='narrow') if content_td else None\n",
        "            post_text = content_div.get_text(separator=' ', strip=True) if content_div else \"\"\n",
        "\n",
        "            like_text = content_td.find('p', class_='s') if content_td else None\n",
        "            likes, shares = 0, 0\n",
        "            if like_text:\n",
        "                like_match = re.search(r'(\\d+)\\s+Like', like_text.text)\n",
        "                share_match = re.search(r'(\\d+)\\s+Share', like_text.text)\n",
        "                if like_match:\n",
        "                    likes = int(like_match.group(1))\n",
        "                if share_match:\n",
        "                    shares = int(share_match.group(1))\n",
        "\n",
        "            posts.append({\n",
        "                'username': username,\n",
        "                'gender': gender,\n",
        "                'timestamp': timestamp,\n",
        "                'post_text': post_text,\n",
        "                'likes': likes,\n",
        "                'shares': shares,\n",
        "                'page_number': page_num,\n",
        "                'post_url': url\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing post on page {page_num}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return posts"
      ],
      "metadata": {
        "id": "WjzicJbdz6qA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running the above helper function on all 1020 pages and then saving as a .csv file"
      ],
      "metadata": {
        "id": "gIEf4LSV0Qb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_posts = []\n",
        "for page in tqdm(range(1, 1021), desc=\"Scraping all pages\"):\n",
        "    success = False\n",
        "    attempts = 0\n",
        "    while not success and attempts < 3:\n",
        "        page_posts = parse_page(page)\n",
        "        if page_posts:\n",
        "            all_posts.extend(page_posts)\n",
        "            success = True\n",
        "        else:\n",
        "            attempts += 1\n",
        "            print(f\"Retrying page {page}... attempt {attempts}\")\n",
        "            time.sleep(5 + random.uniform(1.5, 3.5))\n",
        "\n",
        "    time.sleep(random.uniform(2, 4))\n",
        "\n",
        "# converting to dataframe so that can save to .csv later\n",
        "df = pd.DataFrame(all_posts)\n",
        "\n",
        "# saving as .csv file type\n",
        "df.to_csv(\"nairaland_bbnaija_2024.csv\", index=False, encoding='utf-8-sig')\n",
        "print(\"Saved to nairaland_bbnaija_2024.csv\")\n"
      ],
      "metadata": {
        "id": "3rSnHP43z6ko"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}